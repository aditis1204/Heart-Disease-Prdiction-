# -*- coding: utf-8 -*-
"""ML_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K6-Jflx0bb08p0FjTg6MPXYoQ-R8aSVS

To read more about Datset can be found at http://archive.ics.uci.edu/ml/datasets/Heart+Disease

## Import Libraries and Dataset
"""

import sys
import pandas as pd
import numpy as np
import sklearn
import matplotlib
import keras
import seaborn as sns

import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix

# import the heart disease dataset
url = "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

# the names will be the names of each column in our pandas DataFrame
names = ['age',
        'sex',
        'cp',
        'trestbps',
        'chol',
        'fbs',
        'restecg',
        'thalach',
        'exang',
        'oldpeak',
        'slope',
        'ca',
        'thal',
        'class']

# read the csv
cleveland = pd.read_csv(url, names=names)
data=cleveland



print ((cleveland.shape))
print (cleveland.loc[1])

cleveland.loc[280:]

"""### Data Cleaning:

Data Cleaning: Drop rows with "?"
"""

data = data[~data.isin(['?'])]

data = data.dropna(axis=0)
data.shape

print (data.shape)
print (data.dtypes)

"""# Data Visualization:

Pie Chart and Bar Plot to understand frequency for each class:
"""

# create two plots side by side
f, ax = plt.subplots(1,2,figsize=(14,6))
data['class'].value_counts().plot.pie(autopct='%1.2f%%', ax=ax[0], shadow=True)
ax[0].set_title('Class')
ax[0].set_ylabel('')
sns.countplot('class', data=data, ax=ax[1])
plt.show()

"""Plotting Histograms:"""

data.hist(figsize = (20, 12))
plt.show()

"""create swarmplot inside the violinplot for each class:"""

plt.figure(figsize=(13,7))
plt.subplot(121)
sns.violinplot(x="class", y="thalach", data=data, inner=None)
sns.swarmplot(x='class', y="thalach", data=data, color='w', alpha=0.5)
plt.subplot(122)
sns.swarmplot(x='class', y='age', data=data)
plt.show()

"""Heatmap Generation:"""

plt.figure(figsize = (14, 10)) 
sns.heatmap(data.corr(), cmap='Reds',annot=True, linecolor='Yellow', linewidths=2.0)
plt.show()

"""catplot between sex and distribution of target class frequency based on sex:"""

#plt.figure(figsize = (50, 20)) 

sns.catplot(data=data, kind='count', x='sex',hue='class',height=5, aspect=2)
plt.show()

"""catplot between distribution of target class frequency based on number of major vessels (0-3) colored by flourosopy:"""

sns.catplot(data=data, kind='count', x='ca',hue='class',height=6, aspect=2)
plt.figure(figsize = (50, 20)) 

plt.show()

"""catplot between distribution of target class frequency based on chest pain type"""

sns.catplot(data=data, kind='count', x='cp',hue='class',height=6, aspect=2)
plt.xlabel("1: typical angina          2: atypical angina          3: non-anginal pain          4: asymptomatic")
plt.figure(figsize = (50, 20)) 

plt.show()

"""Scatter Plot"""

# create another figure

plt.figure(figsize=(10,6))

# scatter with positive examples
plt.scatter(data.age[data.sex==0],
            data.trestbps[data.sex==0],
            c="salmon");

# scatter with negative exaples
plt.scatter(data.age[data.sex==1],
            data.trestbps[data.sex==1],
            c="lightblue");
plt.title("Scatter plot between Age and Max Heart Rate with respect to sex")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Male", "Female"]);

"""crosstab creation:"""

# Making it more visual

pd.crosstab(data.cp,data.sex).plot(kind="bar", figsize=(10,6), color=["salmon","lightblue"])

plt.title("Distribution of Chest Pain type based on sex")
plt.xlabel("1: typical angina     2: atypical angina     3: non-anginal pain     4: asymptomatic")
plt.ylabel("No of people")
plt.legend(["Male", "Female"]);
plt.xticks(rotation=0);

pd.crosstab(data.fbs, data.sex).plot(kind="bar", figsize=(10,6), color=["salmon","lightblue"])
plt.title("Distribution of Fasting sugar Frequency based on Sex")
plt.xlabel("0 = False, 1 = True")
plt.ylabel("No of people")
plt.legend(["Female","Male"]);
plt.xticks(rotation=0);



"""Plotting catplot with two variables:"""

sns.catplot(data=data, kind='count', x='class', col='sex',row='slope', palette='Reds')
plt.show()

sns.catplot(data=data, kind='count', x='class', col='sex',row='exang', palette='Blues')
plt.show()

"""Plotting countplot:"""

sns.countplot(data["ca"])

sns.countplot(data["sex"])

sns.countplot(data["fbs"])

sns.countplot(data["restecg"])

sns.countplot(data["exang"])

sns.countplot(data["slope"])

sns.countplot(data["thal"])

"""Plotting univariate distributions:"""

sns.distplot(data["cp"])

sns.distplot(data["ca"])

sns.distplot(data["thal"])

sns.distplot(data["slope"])

sns.distplot(data["oldpeak"])

y = data["sex"]

sns.barplot(data["exang"],y)

sns.barplot(data["ca"],y)

sns.barplot(data["exang"],y)

sns.barplot(data["slope"],y)

"""### Algorith Implementation:

Neural Network:
"""

# create X and Y datasets for training
from sklearn import model_selection

X = np.array(data.drop(['class'], 1))
y = np.array(data['class'])

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)

# convert the data to categorical labels
from keras.utils.np_utils import to_categorical

Y_train = to_categorical(y_train, num_classes=None)
Y_test = to_categorical(y_test, num_classes=None)
print (Y_train.shape)
print (Y_train[:10])



from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

# define a function to build the keras model
def create_model():
    # create model
    model = Sequential()
    model.add(Dense(8, input_dim=13, kernel_initializer='normal', activation='relu'))
    model.add(Dense(4, kernel_initializer='normal', activation='relu'))
    model.add(Dense(5, activation='softmax'))
    
    # compile model
    adam = Adam(lr=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
    return model

model = create_model()

print(model.summary())

model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose = 1)

"""To improve accuracy convert into binary classification problem - heart disease or no heart disease"""

Y_train_binary = y_train.copy()
Y_test_binary = y_test.copy()

Y_train_binary[Y_train_binary > 0] = 1
Y_test_binary[Y_test_binary > 0] = 1

print (Y_train_binary[:20])

# define a new keras model for binary classification
def create_binary_model():
    # create model
    model = Sequential()
    model.add(Dense(16, input_dim=13, kernel_initializer='normal', activation='relu'))
    model.add(Dense(8, kernel_initializer='normal', activation='relu'))
    model.add(Dense(4, kernel_initializer='normal', activation='relu'))

    model.add(Dense(1, activation='sigmoid'))
    
    # Compile model
    adam = Adam(lr=0.001)
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
    return model

binary_model = create_binary_model()

print(binary_model.summary())

# fit the binary model on the training data
binary_model.fit(X_train, Y_train_binary, epochs=100, batch_size=10, verbose = 1)

from sklearn.metrics import classification_report, accuracy_score

categorical_pred = np.argmax(model.predict(X_test), axis=1)

print('Results for Categorical Model')
print(accuracy_score(y_test, categorical_pred))
print(classification_report(y_test, categorical_pred))



# generate classification report using predictions for binary model 
binary_pred = np.round(binary_model.predict(X_test)).astype(int)

print('Results for Binary Model')
print(accuracy_score(Y_test_binary, binary_pred))
print(classification_report(Y_test_binary, binary_pred))



"""## KNN Algorithm"""

url = "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

# the names will be the names of each column in our pandas DataFrame
names = ['age',
        'sex',
        'cp',
        'trestbps',
        'chol',
        'fbs',
        'restecg',
        'thalach',
        'exang',
        'oldpeak',
        'slope',
        'ca',
        'thal',
        'class']

# read the csv
datset = pd.read_csv(url, names=names)

datset

# remove null values as less values
data = cleveland[~cleveland.isin(['?'])]    
#dropping row
data = data.dropna(axis=0)

print (data.shape)
print (data.dtypes)

data = data.apply(pd.to_numeric)
data.dtypes
# print (data.shape)
# print (data.dtypes)
# print(data)
colnames_numeric = data.columns[0:14]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data[colnames_numeric] = scaler.fit_transform(data[colnames_numeric])
X = np.array(data)
y = np.array(data['class'])

#print(X)
# y

y[y>0]=1

data['class']=y
#data1 = data

df = data.values.tolist()

#Breaking the data into training and test set
import random
def train_test_split(data, split, trainingSet = [], testSet = []):
    for x in range(len(data)):
        if random.random() < split:
            trainingSet.append(data[x])
        else:
            testSet.append(data[x])

trainingSet = []
testSet = []
split = 0.8
train_test_split(df, split, trainingSet, testSet)

len(trainingSet)

len(testSet)

#Define Euclidean distances
import math
def Euclideandist(x,xi, length):
    d = 0.0
    for i in range(length):
        d += pow(float(x[i])- float(xi[i]),2)
    return math.sqrt(d)

#Getting the K neighbours having the closest Euclidean distance to the test instance
import operator
def getNeighbors(trainingSet, testInstance, k):
    distances = []
    length = len(testInstance)-1
    for x in range(len(trainingSet)):
        dist = Euclideandist(testInstance, trainingSet[x], length)
        distances.append((trainingSet[x], dist))
    distances.sort(key=operator.itemgetter(1))
    neighbors = []
    for x in range(k):
        neighbors.append(distances[x][0])
    return neighbors

#After sorting the neighbours based on their respective classes, max voting to give the final class of the test instance
import operator
def getResponse(neighbors):
	classVotes = {}
	for x in range(len(neighbors)):
		response = neighbors[x][-1]
		if response in classVotes:
			classVotes[response] += 1
		else:
			classVotes[response] = 1
	sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)#Sorting it based on votes
	return sortedVotes[0][0] #Please note we need the class for the top voted class, hence [0][0]#

#Getting the accuracy
def getAccuracy(testSet, predictions):
	correct = 0
	for x in range(len(testSet)):
		if testSet[x][-1] == predictions[x]:
			correct += 1
	return (correct/(len(testSet))) * 100.0

# generate predictions
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report
predictions=[]
actual = []
k = 3
for x in range(len(testSet)):
    neighbors = getNeighbors(trainingSet, testSet[x], k)
    result = getResponse(neighbors)
    predictions.append(result)
    actual.append(testSet[x][-1])
    #print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
#print (predictions)
#print(actual)
results = confusion_matrix(actual, predictions)

print ('Confusion Matrix :')
print(results) 
print ('Accuracy Score :',accuracy_score(actual, predictions)) 
print ('Report : ')
print (classification_report(actual, predictions)) 
accuracy = getAccuracy(testSet, predictions)
print('Accuracy: ' + repr(accuracy) + '%')

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, confusion_matrix, classification_report
import seaborn as sns



nnbc_cm = confusion_matrix(actual,predictions)

ax = sns.heatmap(nnbc_cm,annot=True,cmap='coolwarm',fmt='.0f')
ax.set_title('KNN Confusion Matrix')
ax.set_ylabel('True Label')
ax.set_xlabel('Predicted Label')



"""# LOGISTIC REGRESSION"""

class LogisticRegression:  
    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):  
        self.lr = lr  
        self.num_iter = num_iter  
        self.fit_intercept = fit_intercept  
        self.verbose = verbose  
     
    def __add_intercept(self, X):  
        intercept = np.ones((X.shape[0], 1))  
        return np.concatenate((intercept, X), axis=1)  
     
    def __sigmoid(self, z):  
        return 1 / (1 + np.exp(-z))  
    def __loss(self, h, y):  
        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()  
     
    def fit(self, X, y):  
        if self.fit_intercept:  
            X = self.__add_intercept(X)  
         
        # weights initialization  
        self.theta = np.zeros(X.shape[1])  
         
        for i in range(self.num_iter):  
            z = np.dot(X, self.theta)  
            h = self.__sigmoid(z)  
            gradient = np.dot(X.T, (h - y)) / y.size  
            self.theta -= self.lr * gradient  
             
            z = np.dot(X, self.theta)  
            h = self.__sigmoid(z)  
            loss = self.__loss(h, y)  
                 
            if(self.verbose ==True and i % 10000 == 0):  
                print(f'loss: {loss} \t')  
     
    def predict_prob(self, X):  
        if self.fit_intercept:  
            X = self.__add_intercept(X)  
     
        return self.__sigmoid(np.dot(X, self.theta))  
     
    def predict(self, X):  
        return self.predict_prob(X).round()

df= pd.read_csv(url, names=names)

# print(df)

# remove null values as less values
df = df[~df.isin(['?'])]    
#dropping row
df = df.dropna(axis=0)

a = pd.get_dummies(df['cp'], prefix = "cp")
b = pd.get_dummies(df['thal'], prefix = "thal")
c = pd.get_dummies(df['slope'], prefix = "slope")
d= pd.get_dummies(df['ca'], prefix = "ca")
frames = [df, a, b, c,d]
df = pd.concat(frames, axis = 1)
df = df.drop(columns = ['cp', 'thal', 'slope', 'ca'])

#15613

df.shape

df['class'].value_counts()

df['class'] = df['class'].apply(lambda x : 1 if x >= 1 else 0)

df = df.apply(pd.to_numeric)

df['class'].value_counts()

Y = df['class']
X_original = df.drop(['class','oldpeak'], axis = 1)



X= (X_original - np.min(X_original)) / (np.max(X_original) - np.min(X_original))

# create X and Y datasets for training
from sklearn import model_selection

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size = 0.3)
#reset index

type(X_train)

y_train.reset_index(inplace=True, drop=True)
y_test.reset_index(inplace=True, drop=True)

model = LogisticRegression(lr=0.001, num_iter=10000)  
get_ipython().magic(u'time model.fit(X_train, y_train)')

preds_test = model.predict(X_test)
preds_train = model.predict(X_train)

q = pd.DataFrame(np.around(preds_test)) #prediciton
#print(q)

q['train']=pd.DataFrame(np.around(preds_train))
q['test class']=y_test
#print(q)



q['class train']=y_train
#print(q)
print('Test Accuracy:',q.loc[q[0]==q['test class']].shape[0] / q.shape[0] * 100)

from sklearn import metrics
print('Test Precision Score: ',metrics.precision_score(y_test, preds_test))
print('Test Recall Score: ',metrics.recall_score(y_test, preds_test))
print('Test f1 Score: ',metrics.f1_score(y_test, preds_test))

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, confusion_matrix, classification_report
import seaborn as sns
print(classification_report(y_test, preds_test))


lr_cm = confusion_matrix(y_test, preds_test)

ax = sns.heatmap(lr_cm,annot=True,cmap='coolwarm',fmt='.0f')
ax.set_title('Logistic Regression Confusion Matrix')
ax.set_ylabel('True Label')
ax.set_xlabel('Predicted Label')



"""# SVM"""

import numpy as np
import math

'---------------------------------------------------------'


# Functions

def threshold(array):
    for i in range(len(array)):
        if array[i] > 0:
            array[i] = 1
    return array


'---------------------------------------------------------'


# class

class SVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, iterations=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.iterations = iterations
        self.w = None
        self.b = None

    def fit(self, x_train, y_train):
        y_ = np.where(y_train <= 0, -1, 1)
        numberOfSamples, numberOfFeatures = x_train.shape
        # print(x_train)

        self.w = np.zeros(numberOfFeatures)
        # print(self.w)
        self.b = 0

        for _ in range(self.iterations):
            for index, x_i in enumerate(x_train):
                # print('yindex--',y_[index])
                # print('x_i--', x_i)
                # print()
                # print('dot--', np.dot(x_i,self.w))
                condition = y_[index] * (np.dot(x_i, self.w)) - self.b
                # print('condition--',condition)
                # print('x_i.shape',x_i.shape)
                # print('y_index.shape',y_[index].shape)
                # print(y_[index])

                if condition >= 1:
                    self.w = self.w - self.lr * (2 * self.lambda_param * self.w)
                else:
                    self.w = self.w - self.lr * (2 * self.lambda_param * self.w - (x_i * y_[index]))
                    self.b = self.b - self.lr * y_[index]

    def predict(self, x_test):
        linear_output = np.dot(self.w, x_test) - self.b
        return np.sign(linear_output)

data = np.genfromtxt(url, delimiter=',')

t = 100  # <-------------------------------------no. of training samples
f = [2,3,5,7,11,13] # <-----------------feature selection

x_train = data[0:t, f]

y1_train = data[0:t, 13:14]
y_train = threshold(y1_train)
#print(y1_train)

x_test = data[t + 1:301, f]
y1_test = data[t + 1:301, 13:14]
y_test = threshold(y1_test)

svm = SVM()
svm.fit(x_train, y_train)

print('weight--', svm.w)

w1 = np.reshape(svm.w, (len(f), 1))

y_pre = []
sum = 0
count = 0
for i in range(len(y1_test)):
    sum = sum + (np.where((np.dot(x_test[i], w1)) <= 0, 0, 1))

    if (np.where((np.dot(x_test[i], w1)) <= 0, 0, 1)) == y_test[i]:
        count = count + 1

    #print("actual: ", y_test[i], "predicted: ", (np.where((np.dot(x_test[i], w1)) <= 0, 0, 1)))
    y_pre.append(np.where((np.dot(x_test[i], w1)) <= 0, 0, 1))

print('count', count)

print('SVM TEST Accuracy using selected features=', 100 * count / len(y1_test))

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, confusion_matrix, classification_report
import seaborn as sns
print(classification_report(y_test, y_pre))


lr_cm = confusion_matrix(y_test, y_pre)

ax = sns.heatmap(lr_cm,annot=True,cmap='coolwarm',fmt='.0f')
ax.set_title('SVM Confusion Matrix')
ax.set_ylabel('True Label')
ax.set_xlabel('Predicted Label')

